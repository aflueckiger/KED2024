---
title: The ABC of Computational Text Analysis
subtitle: "#9 Ethics and the Evolution of NLP"
author: "Alex Flückiger"
institute: "Faculty of Humanities and Social Sciences<br>University of Lucerne" 
date: "20 April 2024"
lang: en-US
---




## Recap last lecture

- perform first real-world (data) analysis :bar_chart:
  - analyse discourse on wokeness in Swiss media

## Outline

-   ethics is everywhere :see_no_evil::hear_no_evil::speak_no_evil:
    -   ... and your responsibility
-   understand the development of modern NLP :rocket:
    -   ... or how to put words into computers

::: notes
-   heute ein Sprung, der über praktischen Teil von Seminar hinausgeht
    -   Mix aus Ethik und Entwicklung NLP
    -   moderne NLP leistungsfähiger als je zuvor, aber mit Problemen
-   AI ist Werkzeug, erstmal weder gut noch schlecht
    -   Vergleich I: Motor für Krankenwagen oder Panzer (Verteidigung/Angriff).
    -   Vergleich II: Internet für Wikipedia oder für Kinderpornographie
-   Die Frage ist: Wer profitiert genau von was? Wer verliert dabei?
:::

# **Ethics** is more than philosophy.<br>It **is everywhere**.

# An Example

## [You are applying for a job at a big company]{style="color:var(--green);"} {data-background-image="../images/cv.png"}

::: notes
-   Ethik ist nicht nur abstrakt und gehört nicht nur in Philosophie
    -   nicht Begriff ist wichtig, sondern Denkart
    -   nachdenken über Ausgangslage + Konsequenzen
-   Anekdoten aus eigenem Bewerbungsprozess
    -   als Bewerber
    -   für Jobs, die Tools zur automatischen CV-Verarbeitung machen
:::

## Does your CV pass the automatic pre-filtering? {.r-fit-text}

### :red_circle: :green_circle:

::: notes
-   automatische Vorselektion Bewerbungen
-   Aus welchen Gründen vorselektiert oder eben auch nicht?
-   bestenfalls: naiv, schlechtensfalls: anti-liberal/diskriminierend
:::

## Your interview is recorded. :sunglasses: :hot_face:<br> What personal traits are inferred from that? <br>

![Face impressions as perceived by a model by [@Peterson2022]](../images/face_impressions.jpg){width="80%"}

::: notes
-   Smartness korreliert mit Alter und Haarfarbe
-   Trustworthyness korrelliert mit sonnengebräunter Haut
-   Doch was sagen Äusserlichkeiten tatsächlich über psychologische Eigenschaften aus?
    -   Viel Pseudowissenschaft, auch von Topuniversitäten
:::

## Don't worry about the future... {data-background="var(--red)"}

### ... worry about the present.

-   AI is persuasive in everyday's life
    -   assessing risks and performances (credits, job, crimes, terrorism etc.)
-   AI is extremely capable
-   AI is smart within limits only and often poorly evaluated

. . .

:bulb: What is going on behind the scene?

::: notes
-   je mehr die Systeme können, desto mehr werden sie eingesetzt, desto unsichtbarer wird, was sie nicht können
-   Moderne AI lernt Muster aus Daten. Gilt auch für NLP.
    -   generalisiert blind oder eben genau so wie gelernt
-   Ungleichheit wird reproduziert, gar verstärkt durch Systematik
    -   Geschlecht, Ethnie, sozioökonomisch
-   Problematik ist Reichweite und Intransparenz
    -   Menschen auch fallibel, aber mit grösser Varietät
-   Anwendungen für tabelarische Daten, Text, Bild, Video
    -   self-driving cars (walking pedestrians vs wheel chair pedestrians)
-   Entwicklung NLP und Ethik hängen zusammen
    -   quasi: je leistungsfähiger NLP, desto mehr Bias wird mitgelernt im aktuellen Paradigma
    -   besseres Verständnis = bessere Data Science
:::

# An (R)evolution of NLP

## From Bag of Words to Embeddings

### Putting Words into Computers [@Smith2020; @Church2021; @Manning2022]

-   from **coarse+static** to **fine+contextual** meaning
-   how to measure similarity of words and documents?
-   from counting to learning representations

::: notes
-   Ein Sprung, wie diese Technologie gewachsen ist
-   Probleme
    -   Bank kann Kreditinstitut bedeuten, in anderem Satz Parkbank
    -   Haus und Gebäude sehr ähnlich, aber nicht reflektiert in Oberflächenform
:::

## Bag of Words

-   word as arbitrary, discrete numbers
    -   `King = 1, Queen = 2, Man = 3, Woman = 4`
-   intrinsic meaning
-   how are these words similar?

![Vector-representations of words as discrete symbols [@Colyer2016]](../images/word2vec-one-hot.png)

::: notes
-   Vektorrepräsentation für ganzes Vokubular
-   jedes Wort ist anderes als jedes andere, in unvergleichbarer Weise
-   BoW lange Zeit Standard. Ergänzt durch zusätzliche Information wie POS
:::

## Representing a Corpus

::: columns
::: {.column width="50%"}
### Collection of Documents

::: l-left-align
1.  `NLP is great. I love NLP.`

2.  `I understand NLP.`

3.  `NLP, NLP, NLP.`
:::
:::

::: {.column width="50%"}
### Document Term Matrix

|              | `NLP` | `I` | `is` | ***term***           |
|--------------|-------|-----|------|----------------------|
| **Doc 1**    | 2     | 1   | 1    | ...                  |
| **Doc 2**    | 1     | 1   | 0    | ...                  |
| **Doc 3**    | 3     | 0   | 0    | ...                  |
| ***Doc ID*** | ...   | ... | ...  | ***term frequency*** |
:::
:::

::: notes
-   für den Computer müssen Daten tabularisiert werden für weitere Verarbeitung
:::

## "I eat a hot `___` for lunch." {data-background="var(--blue)"}

::: notes
-   Es gibt einen Ausweg
-   Wörter können aber auch anders definiert werden und das will ich hier illustrieren
-   Frage an Studis
:::

## 

> You shall know a word by the company it keeps!
>
> @Firth1957

::: notes
-   kontextuelle Bedeutung statt intrinschische Definition
-   Saussure: Zeichen nur definiert durch andere Zeichen
-   relationale Bedeutung: Objekt ist definiert durch Kontext
-   linguistische Theorie blieb lange ohne technische Implementation
:::

## Word Embeddings

### word2vec [@Mikolov2013]

-   words as continuous vectors
    -   accounting for similarity between words
-   semantic similarity
    -   `King – Man + Woman = Queen`
    -   `France / Paris = Switzerland / Bern`

::: l-multiple
![Single continuous vector per word [@Colyer2016]](../images/word_vectors.png)

![Words as points in a semantic space [@Colyer2016]](../images/word2vec-king-queen-vectors.png)

![Doing arithmetics with words <br>[@Colyer2016]](../images/word2vec-king-queen-composition.png)
:::

::: notes
-   Seit 2013 hat sich alles verändert
-   vector = list of numbers -\> point in Euclidean space
-   Idee: wenn Wort genau gleich gebraucht wird, dann selbe Stelle
-   Synonyme, Analogien finden
-   alles noch globale Information. Ein Wort hat genau ein Vektor
    -   Was passiert mit mehrdeutigen Wörter (z.B. Bank)?
-   Frage wie diese Repräsentationen genau gelernt wir nach Pause
:::

## Contextualized Word Embeddings

### BERT [@Devlin2019]

-   recontextualize static word embedding
    -   different embeddings in different contexts
    -   accounting for ambiguity (e.g., `bank`)
-   acquire linguistic knowledge from language models (LM)
    -   LM predict next/missing word
    -   pre-trained on loads of data

<br>

:boom: embeddings are the cornerstone of modern NLP

::: notes
-   alles lässt sich embedden (Wörter, Sätze, Paragraphen, Dokumente)
:::

## Large Language Models (LLM)

### ChatGPT [@OpenAI2023]

-   scale up attempts of previous models
    -   more model parameters (\>175B) and train data (\>300B words)
-   optimize for conversations
    -   instruction-tuning (summarize, translate, reason)
    -   Reinforcement Learning from Human Feedback (RLHF)

:nerd_face: There are dozens other models than ChatGPT.

::: notes
-   ChatGPT ist bekanntester Vertreter (gibt mehr) und hat historische Vorläufer
-   Was hat sich geändert?
    -   Skalierung, Instruktionen, Automatisierung über Reinforcement
-   Number of words of Wikipedia: 4B
-   RLHF
    -   Wenn man einmal ein gutes Modell hat, kann man verschiedene Outputs generieren lassen und Menschen bewerten lassen
    -   Anderes Modell entwickeln, das menschliche Bewertungen automatisiert
    -   Loop
-   öffentliche Plattform führt zu sehr vielen weiteren Daten
-   **Pause**
:::

# Modern NLP is propelled by Data

## Associations in Data

</br>

<p>[«]{style="font-family:Courier New,Courier,monospace"}[<code>\_\_\_</code>]{style="color:#e74c3c"} becomes a doctor.»</p>

## Learning Patterns from Data

![Gender bias of the commonly used language model [BERT](https://pair.withgoogle.com/explorables/fill-in-the-blank) [@Devlin2019]](../images/bert_bias_doctor.png)

::: notes
-   BERT wird in Google Search gebraucht
:::

## Cultural Associations in Training Data

![Gender bias of the commonly used language model [BERT](https://pair.withgoogle.com/explorables/fill-in-the-blank) [@Devlin2019]](../images/bert_bias_hobbies.png){width="70%"}

::: notes
-   Analyse umkehren: Nicht nach Pronomen fragen, sondern nach Tätigkeiten
-   Model trained on Wikipedia and Books (not Reddit)
:::

## Word Embeddings are biased ...

### ... because ~~**our data** is~~ we are biased. [@Bender2021]

::: notes
-   Timnit Gebru (Google Ethics Lead) gefeuert für dieses Paper Ende 2020
-   Daten sind nicht besser als wir und Gesellschaft trägt extreme Diskriminierungen mit sich
-   Mit Technologie wird Bias aber wieder systematisch in Gesellschaft zurückgeführt
:::

## In-class: Exercises I {data-background="var(--blue)"}

1.  Open the following website in your browser: <https://pair.withgoogle.com/explorables/fill-in-the-blank/>
2.  Read the the article and play around with the interactive demo.
3.  What works surprisingly well? What looks flawed by societal bias? Where do you see limits of large language models?

# Modern AI = DL

## How does Deep Learning look like?

![Simplified illustration of a Neural Network. Arrows are weights.](../images/neural_network.png)

::: notes
-   Ausgangslage sind Input-Output-Paare
-   Modell wird supervisiert gelernt um die Beziehung zwischen Input/Output zu lernen
-   Wie wird dieses Lernen gemacht
:::

## How does Deep Learning work?

### Deep Learning [works](https://thinc.ai/docs/backprop101) like a huge bureaucracy

1.  **start** with **random** prediction
2.  **blame** units for contributing to **wrong predictions**
3.  **adjust** units based on the accounted blame
4.  **repeat** the cycle

. . .

:nerd_face: train with **gradient descent**, a series of **small steps** taken **to minimize an error function**

## Current State of Deep Learning

### Extremely powerful but ... [@Bengio2021]

-   great at **learning patterns**, yet reasoning in its infancy
-   requires tons of data due to inefficient learning
-   generalizes poorly

::: notes
-   out of domain (schwarze vs weisse Menschen, anderes Textgenre)
:::

## Limitations of data-driven Deep Learning

<br>

<p>[„This sentence contains]{style="font-family:Courier New,Courier,monospace"}[37]{style="color:#3498db"} characters.“<br /> „Dieser Satz enthält [32]{style="color:#e74c3c"} Buchstaben.“<br />  </p>

![](../images/chatgpt_sentence_length.png){heigth="6cm"}

::: notes
-   ohne Trainingsdaten aktuell nicht zu lösen
:::

## Doubts about practical implications?

![Gender bias in [Google Translate](https://translate.google.com/?sl=en&tl=de&text=Your%20flatmate%20is%20smart.%0AYour%20flatmate%20is%20beautiful.%0A%0AThe%20engineer%20gets%20a%20promotion.%0AThe%20child%20carer%20goes%20to%20the%20zoo%20with%20the%20kids.%0AThe%20child%20carer%20gets%20a%20promotion.%0A%0A&op=translate)](../images/google_translate_bias.png){width="40%"}

::: notes
-   Stereotypen stecken auch in Produkten
-   Bilder oder Websites werden dadurch möglicherweise schlechter gefunden
:::

# Biased Data and beyond

## 

> Raw data is an oxymoron.
>
> @Gitelman2013

::: notes
-   Was sind Daten?
    -   Daten sind kein Abbild der Welt, nichts natürliches.
-   Analog zu Romanos: Massenmedien sind nicht die Welt
    -   Daten sind auch nicht die Welt
-   Daten liegen nicht einfach herum, sondern gemacht
-   Die Frage was Daten genau sind hat mit Realismus/Konstruktivismus-Debatte zu tun
    -   Konstruktivmus heisst nur, Fragen zu stellen, wieso die Dinge sind, wie sie ausschauen
:::

## Three Sides of the AI Coin

### Explaining vs Solving vs Tracking

-   conduct research to **understand**
-   **automate** tedious tasks
-   **track** people for profit or political reasons

## Fair is a Fad

-   companies also engage in fair AI to avoid regulation
-   **Fair** and **good** – **but to whom?** [@Kalluri2020 ]
-   lacking democratic legitimacy

::: notes
-   Fair kann ziemlich vieles bedeuten, solange man es selbst definieren kann
    -   demokratische Legitimität fehlt für all diese Systeme
-   looking beyond data
    -   invading privacy
    -   economic monopolies
    -   (unpaid) AI-trainers and click-workers
    -   environmental costs
:::

## 

> Don’t ask if artificial intelligence is good or fair, ask how it shifts power.
>
> @Kalluri2020

## Data represents real life. {data-background="#4d7e65"}

Don't be a fool. Be wise, think twice.

## Algorithmic Managment of Labour Force

![Text generation may be used to communicate difficult decisions strategically](../images/chatgpt_headcount_reduction.png)

::: notes
-   Akkordarbeit und bei ungenügender Leistung automatische Entlassung?
-   In Amazon Logistikzentren kommt auto-management zum Einsatz
-   sozial wahrscheinlich als perfide, unmoralische Kommunikation aufgefasst
:::

# Questions? {.white-text data-background-image="../images/paint-anna-kolosyuk-unsplash.jpg"}

## References {.scrollable}