---
title: The ABC of Computational Text Analysis
subtitle: "#5 Basic NLP with Command-line"
author: "Alex Flückiger"
institute: "Faculty of Humanities and Social Sciences<br>University of Lucerne" 
date: "23 March 2023"
lang: en-US
---

## Recap last Lecture

- perform shell commands
  - navigate filesystem :evergreen_tree:
  - create/copy/move/remove files :joystick:
  - pipe output into other command :fast_forward:
- complete assignment :writing_hand:



::: notes

- Einstieg in Shell
  - Verzeichnisbaum ist nicht nur Metapher, sondern auch technisch Begriff
  - Erstellen von Files/Ordner
  - Piping für komplexere Operationen
- letztes Mal inhaltliche Zumutung, heute erste inhaltlich interessante Analysen
- Pfade sehr wichtig
- Übungen ok? technische Fragen?

:::

## Outline

- performing corpus linguistic using the shell​ :knife:
  - counting, finding, comparing​​
- analyzing programmes of Swiss parties :bar_chart:



::: notes

- Frequenzanalysen = Schweizer Taschenmesser
  - äusserst effektiv trotz methodischer Einfachheit

- Ziel: mehr Übungszeit
- Syntax nicht merken, Wichtiges werdet ihr schlussendlich erinnern

:::

## When politics changes, <br>language changes.

![Historical development of Swiss party politics ([Tagesanzeiger](https://blog.tagesanzeiger.ch/datenblog/index.php/1791/wie-sich-die-svp-aus-dem-buergerblock-verabschiedet-hat))](../images/swiss_party_politics.gif)



::: notes

- Motivieren der heutigen Sitzung
- Positionierung Parteien im politischen Raum über Zeit
- Gleiche Parteien, aber neue Ziele und Positionierung. Also doch nicht so gleich!
- Wie erkenne ich semantische Veränderungen?
  - hier: Abstimmungsparolen von Parteien ausgewertet
  - Welche Ziele/Ideologien stehen dahinter? --> Texte fundamental
- Wenn Politik ändert, ändert sich Sprache
  - oder gerade umgekehrtes zeitliches Verhältnis / Kausalität
  - in Politik werden Narrative erprobt



Quelle: https://swissvotes.ch

:::



## How to process a Text Collection

1. each document as individual file ​(​`​.​t​x​t​`​)​
   - use shell for quick analysis
2. a dataset of documents (`.csv`, `.tsv`, `.xml`)
   - use Python for in-depth analysis

::: {.r-stack}

![](../images/pile_books_unsplash.jpg){.fragment .fade-out}

![From (physical) artifacts featuring texts to datasets](../images/dataset.png){.fragment .current-visible height=8cm}

:::

::: notes

- Datenextraktion und -zusammenstellung kann mehrstufiger Prozess sein
- Start oft via Kommandozeile, dann Auswertung in Python
  - PDF müssen noch umwandelt werden
  - txt-files erste Stufe bei Datensatzerstellung

- Daten existieren viele, Datensätze eher wenige
- sobald strukturierte Daten (Datensatz) -> Analyse in Python, da sehr viele Tools vorhanden
  - Bei tsv/csv-file je Zelle ein Dokument, in gleicher Zeile auch noch Metadaten
- vorerst arbeiten wir nur mit txt files
  - fördert auch Verständnis


:::



# Counting Things{data-background-image=../images/counting_blackboard.jpg  .white-text}

## Measuring Relevance by Frequency

### Bag of words approach

:::::::::::::: {.columns}

:::{.column width="60%"}

* counting words regardless of context
* simple (and simplistic)
* powerful
* fast

:::

:::{.column width="40%"}

![Representation of text as a bag of words](../images/word_magnets.jpg)



:::

::::::::::::::



::: notes

- Häufigkeit deutet auf eine Form von Relevanz hin
- in Häufigkeitsanalyse sind Worte kontextlos
  - BoW = Sack mit Wörtern
  - Approach schmerzt aus sozialwissenschaftlicher Perspektive
  - Verlust Ambiguitäten/Negationen
  - Nachteil der radikalen Vereinfachung (einfaches Zählen) = auch grösster Vorteil 
- Luhmann: ob über etwas gesprochen wird, ist noch wichtiger als das wie
- ähnlich wie Google Ngram, aber eigene Daten
- Übersicht von besseren Methoden nach Osterpause

:::

## Get Key Figures of Texts

```bash
wc *.txt	# count number of lines, words, characters
```

::: notes

- zuerst Charakterisierung Datenquelle, nicht nur Inhalt
- Zahlen für einzelne Dokumente und aggregiert auf Sammlung

:::

## Get all Word Occurrences

### Show phrase in context

```bash
egrep -ir "data" FOLDER/	# search in all files in given folder

# common egrep options:
# -i 			search case-insensitive
# -r			search recursively in all subfolders
# --colour 		highlight matches
# --context 2 	show 2 lines above/below match
```

::: notes

- options
  - ignore case
  - recursive / specific files
- egrep -ir "data" KED2023/lectures/md/
- Dateinamen als Filter benutzen
  - Quelle/Jahr
  - egrep -ir "data" KED2023/lectures/md/*md

:::



## Count Word Occurrences

### Get counts per file

```bash
egrep -ic "big data" *.txt 		# count across all txt-files, ignore case
```



::: notes

- egrep -irc --colour --context 3  "data"  lectures/md/*md | sort
- PDF zeigen von Parteiprogrammen
- Kurze Öko-Analyse: Wer spricht über Ökologie
  - cd /home/alex/KED2023/KED2023/materials/data/swiss_party_programmes
  - egrep -irc "ökologisch" .
- wc als Alternative

:::



## Word Frequencies

### Steps of the algorithm

1. split text into one word per line (tokenize)
2. sort words alphabetically
3. count how often each word appears

. . .

```bash
# piping steps to get word frequencies
cat text.txt | tr " " "\n" | sort | uniq -c | sort -h > wordfreq.txt

# explanation of individual steps:
tr " " "\n" 	# replace spaces with newline 
sort -h			# sort lines alphanumerically
uniq -c			# count repeated lines
```



::: notes

- Zweck: Häufigkeiten aller Wörter
- kein direkter Befehl -> Kombinieren von Befehlen (modular)
- Befehle erklären
  - Zusammenfassen gleicher Zeilen mit uniq
- Newline Character als Steuerzeichen
- Aggregation extrem flexibel
  - anderer Text, alle Texte (*)
- Frage an Klasse: Was ist häufigstes Wort bei SVP? 
  - Funktionswörter
  - Schweiz, Bürger etc.: national, männlich
  - `cat KED2023/materials/data/swiss_party_programmes/txt/svp_programmes/*txt |  tr " " "\n" | sort | uniq -c | sort -h`

:::



## Two Kinds of Word Frequencies

- absolute frequency
  - `= n_occurrences`

- relative frequency
  - `= n_occurrences / n_total_words`
  - allows to compare corpora of different sizes
- statistical validation of variation
  - significance tests between corpora



::: notes

- Korpus = Textsammlung
- absolut nur, wenn grösserer Output (z.B. mehr Dokumente) mitgemessen werden soll

:::



## Convert Stats into Dataset

- convert to `.tsv` file
- useful for further processing
  - e.g., import in Excel

```bash
# convert word frequencies into tsv-file
# additional step: replace a sequence of spaces with a tabulator
cat text.txt | tr " " "\n" | sort | uniq -c  | sort -h | \
tr -s " " "\t"  > test.tsv	
```

::: notes

- -s alle Leerschläge durch Tabulator ersetzen
- relative frequency in Excel berechnen

:::

## In-class: Matching and counting {data-background=#3c70b5}

1. Print the following sentence in your command line using `echo`.
   ```bash
   echo "There are a few related fields: \
   NLP, computational linguistics, and computational text analysis."
   ```

2. How many words are in this sentence? Use the pipe operator `|` to pass the output above to the command `wc`.

3. Match the words `computational` and colorize its occurences in the sentence using `egrep`.

4. Get the frequencies of each word in this sentence using `tr` and other commands.

5. Save the frequencies into a tsv-file, open it in a spreadsheet programm (e.g., Excel, Numbers) and compute the relative frequency per word.



::: notes

**Pause**

:::



# Preprocessing

## Preprocess your data

### for refining results

* lowercasing
* replace symbols
* join lines
* trimming header + footer
* splitting into multiple files
* using patterns to remove/extract parts :date:



::: notes

- Preprocessing für bessere Resultate

- Regex nächste Woche

:::



## Lowercasing

### Reduce word forms

```bash
echo "ÜBER" | tr "A-ZÄÖÜ" "a-zäöü"	# fold text to lowercase
```



::: notes

- Grossschreibung Satzanfang
- bei Bag of Words macht Grosschreibunga wenig Sinn

:::

## Removing and Replacing Symbols

```bash
echo "3x3" | tr -d "[:digit:]"		# remove all digits	
cat text.txt | tr -d "[:punct:]"	# remove punctuation like .,:;?!- 

tr "Y" "Z"							# replace any Y with Z
```



::: notes

- Es gibt Zeichenklassen für Buchstaben, Zahlen und Interpunktion
- löscht alle Einzelzeichen in Text (keine Sequenzen)
- Interpunktion wird sehr oft entfernt, da sowieso Kontext verloren geht in BoW

::: 



## Standard Preprocessing

### Save a preprocessed document

```bash
# lowercase, no punctuation, no digits
cat speech.txt | tr "A-ZÄÖÜ" "a-zäöü" | \
tr -d "[:punct:]" | tr -d "[:digit:]" > speech_clean.txt
```

::: notes

- Kleinschreibung , keine Interpunktion, keine Zahlen
- standardmässige Repräsentation in BoW (hier noch mit Reihenfolge)

:::

## Join Lines 

```bash
cat test.txt | tr -s "\n" " "	# replace newlines with spaces
```



::: notes

- harte Zeilenumbrüche entfernen
- squeeze repeated newline and replace with a single whitespace

:::

## Trim Lines

```bash
cat -n text.txt			# show line numbers
sed "1,10d" text.txt 	# remove lines 1 to 10
```

## Check Differences between Files

### sanity check after modification

```bash
# show differences side-by-side and only differing lines
diff -y --suppress-common-lines text_raw.txt text_proc.txt
```

## Split Files

```bash
# splits file at every delimiter into a standalone file
csplit huge_text.txt  "/delimiter/" {*}
```

::: notes

- skip when lack of time

::: 

## Where there is a shell,<br>there is a way. :thumbsup: {data-background=#4d7e65} 

::: notes

- Zusammenfassung
  - Nach Filesystem, nun auch Bearbeiten, Zählen 
- Shell =  flexibles + mächtiges Werkzeug durch Kombinieren von mehreren Commands
- Stackoverflow liefert Antworten auf ein Problem

:::

## Organizing Code

- [Git](https://git-scm.com/) tracks file changes and allows for version management
- [GitHub](https://github.com/) is a popular hosting platform based on Git
  - share code and collaborate
  - repository = project folder



:nerd_face: Publishing code and data are key to open science.



::: notes

- Kurzer Sprung, aber ist Basis für in-class Übung
- Version Managment Software
  - ähnlich Änderungsmodus in Word
- Nutzen
  - für moderne Software-Entwicklung nicht wegzudenken
  - neuerdings für Tracking wissenschaftlicher Arbeiten

- Repository = Ablage

:::

## Move to Zoom temporarily :desktop_computer::house_with_garden:

We are having the next to classes via Zoom

- 30 March 2023
- 06 April 2023

# Questions?{data-background="../images/paint-anna-kolosyuk-unsplash.jpg" .white-text}

## In-class: Getting ready {data-background=#3c70b5}

1. Change into your local copy of the GitHub course repository KED2023 and update it with `git pull`. When you haven't cloned the repository, follow section 5 of the [installation guide](https://aflueckiger.github.io/KED2023/materials/installation_guide.pdf) .
2. You find some party programmes (Grüne, SP, SVP) in `KED2023/materials/data/swiss_party_programmes/txt`. Change into that directory using `cd`. 
3. The programmes are provided in plain text which I have extracted from the publicly available PDFs. Have a look at the content of some of these text files using `more`. 

## In-class: Analyzing Swiss Party Programmes I{data-background=#3c70b5}

1. Compare the absolute frequencies of single terms or multi-word phrases of your choice (e.g., Ökologie, Sicherheit, Schweiz)...
   - across parties
   - historically within a party

   Use the file names as filter to get various aggregation of the word counts.

2. Pick terms of your interest and look at their contextual use by extracting relevant passages. Does the usage differ across parties or time?



:bulb: **Share your insights with the class using [Etherpad](https://etherpad.wikimedia.org/p/KED2023).** 

## In-class: Swiss Party Programmes II{data-background=#3c70b5}

1. Convert the word frequencies per party into a `tsv` dataset. Compute the relative word frequency instead of the absolute frequency using any spreadsheet software (e.g. Excel). Are your conclusions still valid after accounting for the size?
2. Can you refine the results with further preprocessing of the data (e.g., stripping punctuation, lowercasing)?
3. What is the size of the vocabulary of this data collection (number of unique words)?



**Pro Tip** :nerd_face:: Use `egrep` to look up commands in the `.md` course slides

## Additional Resources

When you look for useful primers on Bash, consider the following resources:

- [Tutorial Basic Text Analysis by W. Turkel](https://williamjturkel.net/2013/06/15/basic-text-analysis-with-command-line-tools-in-linux/)
- [Tutorial Pattern Matching + KWIC by W. Turkel](https://williamjturkel.net/2013/06/20/pattern-matching-and-permuted-term-indexing-with-command-line-tools-in-linux/)

